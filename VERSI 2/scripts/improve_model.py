"""
IMPLEMENTASI PENINGKATAN AKURASI MODEL
======================================
Script ini mengimplementasikan semua strategi untuk meningkatkan akurasi
dari 40% ke target 60-70%
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
import os
warnings.filterwarnings('ignore')

# Get the directory where this script is located
script_dir = os.path.dirname(os.path.abspath(__file__))
models_dir = os.path.join(script_dir, '..', 'models')
data_dir = os.path.join(script_dir, '..', 'data')
docs_dir = os.path.join(script_dir, '..', 'docs')

print("=" * 80)
print("PENINGKATAN AKURASI MODEL PENETASAN MAGGOT")
print("=" * 80)

# =====================================================
# 1. LOAD DATA ASLI
# =====================================================
print("\n[STEP 1] Loading original data...")
df_original = pd.read_csv(os.path.join(data_dir, 'dummy_data.csv'), delimiter=';')
print(f"âœ“ Data loaded: {len(df_original)} samples, {len(df_original.columns)} columns")

# =====================================================
# 2. DATA AUGMENTATION
# =====================================================
print("\n[STEP 2] Data Augmentation...")

def augment_data(df, target_samples=500):
    """Generate more training data"""
    augmented_rows = []
    needed = target_samples - len(df)
    
    print(f"  Generating {needed} additional samples...")
    
    for i in range(needed):
        # Random sample
        base = df.sample(1).iloc[0].copy()
        
        # Add realistic variations
        base['Jumlah_telur_gram'] *= np.random.uniform(0.85, 1.15)
        base['Makanan_gram'] *= np.random.uniform(0.9, 1.1)
        base['temp'] += np.random.uniform(-3, 3)
        base['humidity'] += np.random.uniform(-8, 8)
        base['temp_max'] = base['temp'] + np.random.uniform(1, 4)
        
        # Clip to realistic ranges
        base['Jumlah_telur_gram'] = np.clip(base['Jumlah_telur_gram'], 50, 500)
        base['Makanan_gram'] = np.clip(base['Makanan_gram'], 1000, 50000)
        base['temp'] = np.clip(base['temp'], 24, 33)
        base['humidity'] = np.clip(base['humidity'], 65, 95)
        base['temp_max'] = np.clip(base['temp_max'], 26, 35)
        
        # Recalculate panen based on pattern
        base['Jumlah_panen_gram'] = base['Makanan_gram'] * np.random.uniform(0.15, 0.25)
        
        augmented_rows.append(base)
        
        if (i+1) % 100 == 0:
            print(f"    Progress: {i+1}/{needed}")
    
    augmented_df = pd.DataFrame(augmented_rows)
    combined = pd.concat([df, augmented_df], ignore_index=True)
    
    print(f"âœ“ Augmentation complete: {len(df)} â†’ {len(combined)} samples")
    return combined

df_augmented = augment_data(df_original, target_samples=500)

# =====================================================
# 3. FEATURE ENGINEERING
# =====================================================
print("\n[STEP 3] Feature Engineering...")

def create_features(df):
    """Create advanced features"""
    df = df.copy()
    
    # Interaction features
    df['temp_humidity_idx'] = (df['temp'] * df['humidity']) / 1000
    df['temp_range'] = df['temp_max'] - df['temp']
    df['telur_per_temp'] = df['Jumlah_telur_gram'] / df['temp']
    
    # Polynomial features
    df['temp_squared'] = df['temp'] ** 2
    df['humidity_squared'] = df['humidity'] ** 2
    
    # Categorical binning
    df['temp_level'] = pd.cut(df['temp'], bins=[0, 26, 29, 100], labels=[0, 1, 2])
    df['humidity_level'] = pd.cut(df['humidity'], bins=[0, 75, 85, 100], labels=[0, 1, 2])
    
    # Condition indicators
    df['optimal_temp'] = ((df['temp'] >= 27) & (df['temp'] <= 30)).astype(int)
    df['optimal_humidity'] = ((df['humidity'] >= 70) & (df['humidity'] <= 80)).astype(int)
    df['optimal_condition'] = df['optimal_temp'] * df['optimal_humidity']
    
    # Weather-based features
    df['is_rainy'] = df['weather_main'].isin(['Rain', 'Thunderstorm']).astype(int)
    df['is_clear'] = (df['weather_main'] == 'Clear').astype(int)
    
    # Season features
    df['is_kemarau'] = (df['season'] == 'Kemarau').astype(int)
    df['is_hujan'] = (df['season'] == 'Hujan').astype(int)
    
    return df

df_enhanced = create_features(df_augmented)
new_features = [col for col in df_enhanced.columns if col not in df_original.columns]
print(f"âœ“ Created {len(new_features)} new features")

# =====================================================
# 4. PREPROCESSING
# =====================================================
print("\n[STEP 4] Preprocessing...")

# Encode categorical
le_media = LabelEncoder()
le_weather = LabelEncoder()
le_season = LabelEncoder()

df_enhanced['Media_Encoded'] = le_media.fit_transform(df_enhanced['Media_Telur'])
df_enhanced['Weather_Encoded'] = le_weather.fit_transform(df_enhanced['weather_main'])
df_enhanced['Season_Encoded'] = le_season.fit_transform(df_enhanced['season'])

# Select features
feature_cols = [
    # Original numeric
    'Jumlah_telur_gram', 'temp', 'humidity', 'temp_max',
    # Original encoded
    'Media_Encoded', 'Weather_Encoded', 'Season_Encoded',
    # New features
    'temp_humidity_idx', 'temp_range', 'telur_per_temp',
    'temp_squared', 'humidity_squared',
    'temp_level', 'humidity_level',
    'optimal_temp', 'optimal_humidity', 'optimal_condition',
    'is_rainy', 'is_clear', 'is_kemarau', 'is_hujan'
]

X = df_enhanced[feature_cols]
y = df_enhanced['Lama_menetas_hari']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ“ Training samples: {len(X_train)}")
print(f"âœ“ Testing samples: {len(X_test)}")
print(f"âœ“ Total features: {len(feature_cols)}")

# =====================================================
# 5. MODEL COMPARISON
# =====================================================
print("\n[STEP 5] Training & Comparing Models...")

models = {
    'Random Forest (Original)': RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        class_weight='balanced'
    ),
    'Random Forest (Deep)': RandomForestClassifier(
        n_estimators=500,
        max_depth=30,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42,
        class_weight='balanced'
    ),
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.1,
        max_depth=7,
        random_state=42
    )
}

results = {}
print("\n" + "-" * 80)

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    # Train
    model.fit(X_train, y_train)
    
    # Evaluate
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    
    results[name] = {
        'model': model,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std()
    }
    
    print(f"  Train Accuracy: {train_acc:.2%}")
    print(f"  Test Accuracy:  {test_acc:.2%}")
    print(f"  CV Score:       {cv_scores.mean():.2%} Â± {cv_scores.std():.2%}")

# =====================================================
# 6. ENSEMBLE MODEL
# =====================================================
print("\n[STEP 6] Creating Ensemble Model...")

ensemble = VotingClassifier(
    estimators=[
        ('rf', models['Random Forest (Original)']),
        ('rf_deep', models['Random Forest (Deep)']),
        ('gb', models['Gradient Boosting'])
    ],
    voting='soft'
)

ensemble.fit(X_train, y_train)
ensemble_train_acc = ensemble.score(X_train, y_train)
ensemble_test_acc = ensemble.score(X_test, y_test)
ensemble_cv_scores = cross_val_score(ensemble, X_train, y_train, cv=5)

results['Ensemble (Voting)'] = {
    'model': ensemble,
    'train_acc': ensemble_train_acc,
    'test_acc': ensemble_test_acc,
    'cv_mean': ensemble_cv_scores.mean(),
    'cv_std': ensemble_cv_scores.std()
}

print(f"  Train Accuracy: {ensemble_train_acc:.2%}")
print(f"  Test Accuracy:  {ensemble_test_acc:.2%}")
print(f"  CV Score:       {ensemble_cv_scores.mean():.2%} Â± {ensemble_cv_scores.std():.2%}")

# =====================================================
# 7. SELECT BEST MODEL
# =====================================================
print("\n" + "=" * 80)
print("RESULTS COMPARISON")
print("=" * 80)

# Sort by test accuracy
sorted_results = sorted(results.items(), key=lambda x: x[1]['test_acc'], reverse=True)

print("\n{:<30} {:<15} {:<15} {:<20}".format(
    "Model", "Train Acc", "Test Acc", "CV Score"
))
print("-" * 80)

for name, res in sorted_results:
    print("{:<30} {:<15} {:<15} {:<20}".format(
        name,
        f"{res['train_acc']:.2%}",
        f"{res['test_acc']:.2%}",
        f"{res['cv_mean']:.2%} Â± {res['cv_std']:.2%}"
    ))

best_model_name = sorted_results[0][0]
best_model = sorted_results[0][1]['model']
best_acc = sorted_results[0][1]['test_acc']

print("\n" + "=" * 80)
print(f"ðŸ† BEST MODEL: {best_model_name}")
print(f"   Accuracy: {best_acc:.2%}")
print("=" * 80)

# =====================================================
# 8. DETAILED EVALUATION
# =====================================================
print("\n[STEP 7] Detailed Evaluation of Best Model...")

y_pred = best_model.predict(X_test)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Feature Importance (if available)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': feature_cols,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nTop 10 Most Important Features:")
    print(feature_importance.head(10).to_string(index=False))
elif hasattr(best_model, 'estimators_'):
    # For ensemble, get average importance
    importances = []
    for est in best_model.estimators_:
        if hasattr(est, 'feature_importances_'):
            importances.append(est.feature_importances_)
    
    if importances:
        avg_importance = np.mean(importances, axis=0)
        feature_importance = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': avg_importance
        }).sort_values('Importance', ascending=False)
        
        print("\nTop 10 Most Important Features (Ensemble Average):")
        print(feature_importance.head(10).to_string(index=False))

# =====================================================
# 9. SAVE IMPROVED MODEL
# =====================================================
print("\n[STEP 8] Saving Improved Model...")

# Save best model
joblib.dump(best_model, os.path.join(models_dir, 'model_penetasan_improved.pkl'))
print("âœ“ Model saved: model_penetasan_improved.pkl")

# Save encoders
joblib.dump(le_media, os.path.join(models_dir, 'label_encoder_media_improved.pkl'))
joblib.dump(le_weather, os.path.join(models_dir, 'label_encoder_weather_improved.pkl'))
joblib.dump(le_season, os.path.join(models_dir, 'label_encoder_season_improved.pkl'))
print("âœ“ Encoders saved")

# Save metadata
metadata = {
    'model_name': best_model_name,
    'feature_columns': feature_cols,
    'test_accuracy': best_acc,
    'cv_mean': sorted_results[0][1]['cv_mean'],
    'cv_std': sorted_results[0][1]['cv_std'],
    'num_training_samples': len(X_train),
    'num_features': len(feature_cols),
    'media_mapping': dict(zip(le_media.classes_, le_media.transform(le_media.classes_))),
    'weather_mapping': dict(zip(le_weather.classes_, le_weather.transform(le_weather.classes_))),
    'season_mapping': dict(zip(le_season.classes_, le_season.transform(le_season.classes_)))
}
joblib.dump(metadata, os.path.join(models_dir, 'model_penetasan_improved_metadata.pkl'))
print("âœ“ Metadata saved")

# Save augmented data
df_enhanced.to_csv(os.path.join(data_dir, 'dummy_data_augmented.csv'), index=False, sep=';')
print("âœ“ Augmented data saved: dummy_data_augmented.csv")

# =====================================================
# 10. VISUALIZATIONS
# =====================================================
print("\n[STEP 9] Creating Visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Model Comparison
ax1 = axes[0, 0]
model_names = [name for name, _ in sorted_results]
accuracies = [res['test_acc'] * 100 for _, res in sorted_results]
colors = ['green' if acc == max(accuracies) else 'steelblue' for acc in accuracies]

ax1.barh(model_names, accuracies, color=colors, alpha=0.8)
ax1.set_xlabel('Test Accuracy (%)')
ax1.set_title('Model Comparison', fontweight='bold', fontsize=12)
ax1.axvline(x=40, color='red', linestyle='--', label='Original (40%)')
ax1.legend()

# 2. Confusion Matrix
ax2 = axes[0, 1]
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2, cbar_kws={'label': 'Count'})
ax2.set_title('Confusion Matrix - Best Model', fontweight='bold', fontsize=12)
ax2.set_xlabel('Predicted')
ax2.set_ylabel('Actual')

# 3. Training History (comparison)
ax3 = axes[1, 0]
x_pos = np.arange(len(model_names))
train_accs = [res['train_acc'] * 100 for _, res in sorted_results]
test_accs = [res['test_acc'] * 100 for _, res in sorted_results]

width = 0.35
ax3.bar(x_pos - width/2, train_accs, width, label='Train', alpha=0.8, color='lightblue')
ax3.bar(x_pos + width/2, test_accs, width, label='Test', alpha=0.8, color='orange')
ax3.set_xticks(x_pos)
ax3.set_xticklabels(model_names, rotation=45, ha='right')
ax3.set_ylabel('Accuracy (%)')
ax3.set_title('Train vs Test Accuracy', fontweight='bold', fontsize=12)
ax3.legend()
ax3.grid(axis='y', alpha=0.3)

# 4. Feature Importance (if available)
ax4 = axes[1, 1]
if 'feature_importance' in locals():
    top_features = feature_importance.head(10)
    colors_fi = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
    ax4.barh(top_features['Feature'], top_features['Importance'], color=colors_fi)
    ax4.set_xlabel('Importance')
    ax4.set_title('Top 10 Feature Importance', fontweight='bold', fontsize=12)
    ax4.invert_yaxis()
else:
    ax4.text(0.5, 0.5, 'Feature importance\nnot available\nfor this model type',
             ha='center', va='center', fontsize=12, transform=ax4.transAxes)
    ax4.set_title('Feature Importance', fontweight='bold', fontsize=12)

plt.tight_layout()
plt.savefig('../docs/model_improvement_results.png', dpi=300, bbox_inches='tight')
print("âœ“ Visualization saved: model_improvement_results.png")

# =====================================================
# 11. SUMMARY
# =====================================================
print("\n" + "=" * 80)
print("IMPROVEMENT SUMMARY")
print("=" * 80)

original_acc = 0.40  # 40%
improvement = (best_acc - original_acc) / original_acc * 100

print(f"""
Original Model:
  - Accuracy: 40%
  - Data: 100 samples
  - Features: 7

Improved Model:
  - Accuracy: {best_acc:.2%}
  - Data: {len(X_train) + len(X_test)} samples
  - Features: {len(feature_cols)}
  - Model: {best_model_name}

Improvement: +{improvement:.1f}% (relative improvement)
Absolute gain: +{(best_acc - original_acc)*100:.1f} percentage points

Key Success Factors:
  âœ“ Data Augmentation ({len(df_original)} â†’ {len(df_enhanced)} samples)
  âœ“ Feature Engineering (+{len(new_features)} features)
  âœ“ Model Comparison (tested {len(models)} + ensemble)
  âœ“ Cross-validation for robust evaluation
""")

print("=" * 80)
print("DONE! Model berhasil ditingkatkan.")
print("=" * 80)
print(f"\nFiles generated:")
print("  - model_penetasan_improved.pkl")
print("  - label_encoder_*_improved.pkl")
print("  - model_penetasan_improved_metadata.pkl")
print("  - dummy_data_augmented.csv")
print("  - model_improvement_results.png")
print("\nGunakan model baru ini untuk prediksi yang lebih akurat!")
